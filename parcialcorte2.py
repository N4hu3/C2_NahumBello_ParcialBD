# -*- coding: utf-8 -*-
"""parcialCorte2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pRfwZzZsG-irYAmemIfj1WUKZ0m238iC
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

df = spark.read.csv("s3://bucket-final/casas/year=2024/month=03/day=15/2024-03-15.csv", inferSchema=True, header=True)

df.show()

df = df.fillna({'Area': df.agg({'Area': 'avg'}).first()[0], 'Bedrooms': df.agg({'Bedrooms': 'avg'}).first()[0]})

df.show()

# Añade esta línea antes de ajustar el modelo para asegurarte de que no hay valores nulos
df = df.na.drop()

# Imprime el esquema y muestra los datos para ver si algo no está siendo procesado correctamente
df.printSchema()
df.show()

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
from pyspark.ml.regression import LinearRegression

# Preprocesamiento de datos numéricos
numericAssembler = VectorAssembler(inputCols=["Area", "Bedrooms"], outputCol="numericFeatures")
scaler = StandardScaler(inputCol="numericFeatures", outputCol="scaledNumericFeatures")

# Transformación de datos categóricos
indexer = StringIndexer(inputCol="Adicional", outputCol="AdicionalIndex")
encoder = OneHotEncoder(inputCols=["AdicionalIndex"], outputCols=["AdicionalVec"])

# Combinar todas las características
featureAssembler = VectorAssembler(inputCols=["scaledNumericFeatures", "AdicionalVec"], outputCol="features")

# Modelo de regresión lineal
lr = LinearRegression(featuresCol="features", labelCol="Price", maxIter=50, regParam=0.1, elasticNetParam=0.5)

# Crear el pipeline
pipeline = Pipeline(stages=[numericAssembler, scaler, indexer, encoder, featureAssembler, lr])

train_data, test_data = df.randomSplit([0.8, 0.2], seed=12345)
model = pipeline.fit(train_data)
train_data.show()
predictions = model.transform(test_data)


# Seleccionar y mostrar las columnas relevantes
selected = predictions.select("Price","prediction")
selected.show()

from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(labelCol="Price", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

evaluator = RegressionEvaluator(labelCol="Price", predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(predictions)
print("R-squared on test data = %g" % r2)

from pyspark.sql.functions import lit
from datetime import datetime

# Obtener el año y mes actual
now = datetime.now()
current_year = now.year
current_month = now.month

# Agregar las columnas 'year' y 'month' al DataFrame
predictions = predictions.withColumn('year', lit(current_year))
predictions = predictions.withColumn('month', lit(current_month))
final_df = predictions.select("Area", "Bedrooms", "Adicional", "Price", "prediction", "year", "month")
# Especificar la ruta en S3 y los detalles de AWS (asegúrate de tener configurado esto)
output_path = "s3://prediccionesparcial2/predicciones/"

# Escribir el DataFrame en S3, particionado por año y mes
final_df.write.partitionBy("year", "month").format("parquet").mode("overwrite").save(output_path)

final_df.printSchema()

